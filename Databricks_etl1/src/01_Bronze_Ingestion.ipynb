{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da80992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define configuration\n",
    "# We use the Unity Catalog (UC) 3-level namespace: catalog.schema.table\n",
    "# 'main' is the default catalog created with the workspace.\n",
    "bronze_catalog = \"main\"\n",
    "bronze_schema = \"bronze\"\n",
    "\n",
    "# Define GCS paths using the External Location.\n",
    "# Databricks knows that 'raw_data_source' points to 'gs:///raw/'\n",
    "#users_gcs_path = \"/Volumes/main/default/raw_data_source/users.json\"\n",
    "#events_gcs_path = \"/Volumes/main/default/raw_data_source/events.json\"\n",
    "# Note: An alternative to using Volumes is reading directly from the external location path:\n",
    "\n",
    "users_gcs_path = \"gs://databricks-demo-bucket-123/raw/users.json\"\n",
    "events_gcs_path = \"gs://databricks-demo-bucket-123/raw/events.json\"\n",
    "# For this to work without Volumes, the cluster's service account must have GCS permissions,\n",
    "# or the External Location must be properly configured. For simplicity, we'll assume the direct path.\n",
    "\n",
    "# Define target table names\n",
    "users_table_name = f\"{bronze_catalog}.{bronze_schema}.users\"\n",
    "events_table_name = f\"{bronze_catalog}.{bronze_schema}.events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad199e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Schema (Database) if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_catalog}.{bronze_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load raw 'users' data\n",
    "# JSON files are often multi-line, so we use this option [43]\n",
    "users_df = spark.read.option(\"multiline\", \"true\").json(users_gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load raw 'events' data\n",
    "events_df = spark.read.option(\"multiline\", \"true\").json(events_gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add Ingestion Metadata (Bronze Layer Best Practice)\n",
    "# This adds columns to track when the data was loaded \n",
    "users_df_with_metadata = users_df.withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "                                .withColumn(\"_source_file\", lit(users_gcs_path))\n",
    "\n",
    "events_df_with_metadata = events_df.withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "                                 .withColumn(\"_source_file\", lit(events_gcs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS main.default.raw_data_source\n",
    "    \"\"\"\n",
    ")\n",
    "users_df_with_metadata = users_df_with_metadata.toDF(\n",
    "    *[col.strip() for col in users_df_with_metadata.columns]\n",
    ")\n",
    "events_df_with_metadata = events_df_with_metadata.toDF(\n",
    "    *[col.strip() for col in events_df_with_metadata.columns]\n",
    ")\n",
    "users_table_name = \"main.bronze.users\"\n",
    "events_table_name = \"main.bronze.events\"\n",
    "users_df_with_metadata.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(users_table_name)\n",
    "\n",
    "events_df_with_metadata.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(events_table_name)\n",
    "\n",
    "display(f\"Successfully wrote to {users_table_name}\")\n",
    "display(f\"Successfully wrote to {events_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
